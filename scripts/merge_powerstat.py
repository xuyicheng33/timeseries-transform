"""
æŒ‰ stationid åˆ†ç»„åˆå¹¶ powerforecast_powerstat æ•°æ®é›†
- æ¯ä¸ª stationid ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹æ–‡ä»¶
- æŒ‰ statistictimeï¼ˆç»Ÿè®¡æ—¶é—´ï¼‰æ’åº
- ä¸åšä»»ä½•æ•°æ®æ¸…æ´—
"""
import pandas as pd
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
SOURCE_DIR = Path(r"E:\å¾…å¤„ç†æ•°æ®\powerforecast_powerstat")
OUTPUT_DIR = SOURCE_DIR / "powerforecast_powerstat_merged"

def detect_encoding(file_path, sample_size=10240):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    import chardet
    try:
        with open(file_path, 'rb') as f:
            raw = f.read(sample_size)
        result = chardet.detect(raw)
        return result.get('encoding', 'utf-8') or 'utf-8'
    except:
        return 'utf-8'


def merge_by_stationid():
    """æŒ‰ stationid åˆ†ç»„åˆå¹¶æ•°æ®"""
    print("="*80)
    print("æŒ‰ stationid åˆ†ç»„åˆå¹¶ powerforecast_powerstat æ•°æ®é›†")
    print("="*80)
    print(f"æºç›®å½•: {SOURCE_DIR}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = sorted(SOURCE_DIR.glob("*.csv"))
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    print()
    
    # è¯»å–æ•°æ®å¹¶æŒ‰ stationid åˆ†ç»„
    print("è¯»å–æ•°æ®å¹¶æŒ‰ stationid åˆ†ç»„...")
    stationid_data = {}  # {stationid: [df1, df2, ...]}
    
    for i, file_path in enumerate(csv_files, 1):
        file_name = file_path.name
        
        try:
            # æ£€æµ‹ç¼–ç 
            encoding = detect_encoding(file_path)
            
            # è¯»å–æ–‡ä»¶
            df = pd.read_csv(file_path, encoding=encoding)
            
            if len(df) == 0:
                print(f"  [{i}/{len(csv_files)}] {file_name} - ç©ºæ–‡ä»¶ï¼Œè·³è¿‡")
                continue
            
            # æŒ‰ stationid åˆ†ç»„
            for stationid, group in df.groupby('stationid'):
                if stationid not in stationid_data:
                    stationid_data[stationid] = []
                stationid_data[stationid].append(group)
            
            print(f"  [{i}/{len(csv_files)}] {file_name} - {len(df):,} è¡Œ, {df['stationid'].nunique()} ä¸ª stationid")
        
        except Exception as e:
            print(f"  [{i}/{len(csv_files)}] {file_name} - âŒ é”™è¯¯: {e}")
    
    if not stationid_data:
        print("\nâŒ æ²¡æœ‰å¯åˆå¹¶çš„æ•°æ®")
        return
    
    print(f"\nâœ… è¯»å–å®Œæˆï¼Œå…± {len(stationid_data)} ä¸ªä¸åŒçš„ stationid")
    print()
    
    # åˆå¹¶å¹¶ä¿å­˜æ¯ä¸ª stationid çš„æ•°æ®
    print("åˆå¹¶å¹¶ä¿å­˜æ¯ä¸ª stationid çš„æ•°æ®...")
    
    for idx, stationid in enumerate(sorted(stationid_data.keys()), 1):
        # åˆå¹¶è¯¥ stationid çš„æ‰€æœ‰æ•°æ®
        stationid_df = pd.concat(stationid_data[stationid], ignore_index=True)
        
        # æŒ‰ statistictime æ’åº
        try:
            stationid_df['statistictime'] = pd.to_datetime(stationid_df['statistictime'])
            stationid_df = stationid_df.sort_values('statistictime').reset_index(drop=True)
        except Exception as e:
            print(f"  âš ï¸  stationid {stationid} æ’åºå¤±è´¥: {e}")
        
        # ä¿å­˜æ–‡ä»¶
        output_file = OUTPUT_DIR / f"powerforecast_powerstat_stationid{stationid}.csv"
        # ç¡®ä¿æ—¶é—´æ ¼å¼å®Œæ•´ä¿å­˜ï¼ˆåŒ…å«æ—¶åˆ†ç§’ï¼‰
        if 'statistictime' in stationid_df.columns:
            stationid_df['statistictime'] = stationid_df['statistictime'].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        stationid_df.to_csv(output_file, index=False, encoding='utf-8')
        
        file_size_kb = output_file.stat().st_size / 1024
        
        # æ˜¾ç¤ºè¿›åº¦
        if idx <= 20 or idx % 10 == 0:
            print(f"  [{idx}/{len(stationid_data)}] stationid {stationid}: {len(stationid_df):,} è¡Œ, {file_size_kb:.2f} KB")
            try:
                print(f"      æ—¶é—´èŒƒå›´: {stationid_df['statistictime'].min()} ~ {stationid_df['statistictime'].max()}")
            except:
                pass
    
    # ç»Ÿè®¡æ€»ç»“
    print("\n" + "="*80)
    print("åˆå¹¶å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print("="*80)
    print(f"æ€» stationid æ•°: {len(stationid_data)}")
    print(f"ç”Ÿæˆæ–‡ä»¶æ•°: {len(stationid_data)}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    total_rows = sum(len(pd.concat(data, ignore_index=True)) for data in stationid_data.values())
    print(f"æ€»è®°å½•æ•°: {total_rows:,} è¡Œ")
    
    print("\nstationid ç»Ÿè®¡ï¼ˆå‰20ä¸ªï¼‰:")
    for idx, stationid in enumerate(sorted(stationid_data.keys())[:20], 1):
        row_count = len(pd.concat(stationid_data[stationid], ignore_index=True))
        print(f"  {idx}. stationid {stationid}: {row_count:,} è¡Œ")
    
    if len(stationid_data) > 20:
        print(f"  ... è¿˜æœ‰ {len(stationid_data) - 20} ä¸ª stationid")
    
    print("="*80)
    print("\nğŸ“‹ æ’åºè¯´æ˜ï¼š")
    print("  - æŒ‰ statistictimeï¼ˆç»Ÿè®¡æ—¶é—´ï¼‰å­—æ®µæ’åº")
    print("  - æ¯ä¸ª stationid ç”Ÿæˆç‹¬ç«‹çš„ CSV æ–‡ä»¶")
    print("="*80)


if __name__ == "__main__":
    try:
        merge_by_stationid()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

