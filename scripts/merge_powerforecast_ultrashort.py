"""
å…¨é‡åˆå¹¶ powerforecast_ultrashort æ•°æ®é›†
- å…ˆæŒ‰ forecast_fromtimeï¼ˆé¢„æµ‹èµ·æŠ¥æ—¶é—´ï¼‰æ’åº
- å†æŒ‰ forecast_totimeï¼ˆé¢„æµ‹ç»“æŸæ—¶é—´ï¼‰æ’åº
- ä¸åšä»»ä½•æ•°æ®æ¸…æ´—
- ç”Ÿæˆä¸€ä¸ªå®Œæ•´æ–‡ä»¶

æ³¨æ„ï¼š
- ä» 20241119 å¼€å§‹åˆå¹¶ï¼Œé¿å…æ—©æœŸæ–‡ä»¶çš„æ¢è¡Œç¬¦é—®é¢˜
"""
import pandas as pd
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# é…ç½®
SOURCE_DIR = Path(r"E:\å¾…å¤„ç†æ•°æ®\powerforecast_ultrashort")
OUTPUT_DIR = SOURCE_DIR / "powerforecast_ultrashort_merged"

# æ—¥æœŸèŒƒå›´é…ç½®
START_DATE = "20241119"  # ä» 11-19 å¼€å§‹ï¼Œé¿å…æ¢è¡Œç¬¦é—®é¢˜

OUTPUT_FILE = OUTPUT_DIR / f"powerforecast_ultrashort_from_{START_DATE}.csv"

def detect_encoding(file_path, sample_size=10240):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    import chardet
    try:
        with open(file_path, 'rb') as f:
            raw = f.read(sample_size)
        result = chardet.detect(raw)
        return result.get('encoding', 'utf-8') or 'utf-8'
    except:
        return 'utf-8'


def merge_all():
    """å…¨é‡åˆå¹¶æ•°æ®"""
    print("="*80)
    print("å¼€å§‹å…¨é‡åˆå¹¶ powerforecast_ultrashort æ•°æ®é›†")
    print("="*80)
    print(f"æºç›®å½•: {SOURCE_DIR}")
    print(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    all_csv_files = sorted(SOURCE_DIR.glob("*.csv"))
    
    if not all_csv_files:
        print("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    # æ ¹æ®æ—¥æœŸèŒƒå›´è¿‡æ»¤æ–‡ä»¶
    csv_files = []
    for f in all_csv_files:
        # ä»æ–‡ä»¶åæå–æ—¥æœŸï¼špowerforecast_ultrashort_20241119.csv
        date_str = f.stem.split('_')[-1]  # è·å–æœ€åä¸€éƒ¨åˆ†
        if date_str >= START_DATE:
            csv_files.append(f)
    
    print(f"âœ… æ‰¾åˆ° {len(all_csv_files)} ä¸ªCSVæ–‡ä»¶")
    print(f"ğŸ“… è¿‡æ»¤åï¼ˆä» {START_DATE} å¼€å§‹ï¼‰: {len(csv_files)} ä¸ªæ–‡ä»¶")
    
    if not csv_files:
        print("âŒ è¿‡æ»¤åæ²¡æœ‰æ–‡ä»¶")
        return
    
    print()
    
    # è¯»å–å¹¶åˆå¹¶æ•°æ®
    print("å¼€å§‹è¯»å–å¹¶åˆå¹¶æ•°æ®...")
    all_data = []
    error_count = 0
    
    for i, file_path in enumerate(csv_files, 1):
        file_name = file_path.name
        
        try:
            # æ£€æµ‹ç¼–ç 
            encoding = detect_encoding(file_path)
            
            # è¯»å–æ–‡ä»¶ï¼ˆä» 11-19 å¼€å§‹çš„æ–‡ä»¶æ ¼å¼æ­£å¸¸ï¼Œå¯ä»¥ç›´æ¥è¯»å–ï¼‰
            df = pd.read_csv(file_path, encoding=encoding)
            
            if len(df) == 0:
                print(f"  [{i}/{len(csv_files)}] {file_name} - ç©ºæ–‡ä»¶ï¼Œè·³è¿‡")
                continue
            
            all_data.append(df)
            
            # æ¯10ä¸ªæ–‡ä»¶æˆ–æœ€åä¸€ä¸ªæ–‡ä»¶æ˜¾ç¤ºè¿›åº¦
            if i % 10 == 0 or i == len(csv_files):
                print(f"  [{i}/{len(csv_files)}] {file_name} - {len(df):,} è¡Œ")
        
        except Exception as e:
            print(f"  [{i}/{len(csv_files)}] {file_name} - âŒ é”™è¯¯: {e}")
            error_count += 1
    
    if not all_data:
        print("\nâŒ æ²¡æœ‰å¯åˆå¹¶çš„æ•°æ®")
        return
    
    print(f"\nâœ… è¯»å–å®Œæˆï¼ŒæˆåŠŸè¯»å– {len(all_data)} ä¸ªæ–‡ä»¶")
    if error_count > 0:
        print(f"âš ï¸  {error_count} ä¸ªæ–‡ä»¶è¯»å–å¤±è´¥")
    print()
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    print("åˆå¹¶æ‰€æœ‰æ•°æ®...")
    merged_df = pd.concat(all_data, ignore_index=True)
    print(f"  åˆå¹¶å: {len(merged_df):,} è¡Œ")
    
    # æ˜¾ç¤ºåˆ—ä¿¡æ¯
    print(f"  åˆ—æ•°: {len(merged_df.columns)}")
    print(f"  åˆ—å: {merged_df.columns.tolist()}")
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆå…ˆæŒ‰ forecast_fromtimeï¼Œå†æŒ‰ forecast_totimeï¼‰
    print("\næŒ‰æ—¶é—´æ’åº...")
    print("  æ’åºè§„åˆ™ï¼šå…ˆæŒ‰ forecast_fromtimeï¼Œå†æŒ‰ forecast_totime")
    try:
        merged_df['forecast_fromtime'] = pd.to_datetime(merged_df['forecast_fromtime'])
        merged_df['forecast_totime'] = pd.to_datetime(merged_df['forecast_totime'])
        merged_df = merged_df.sort_values(
            ['forecast_fromtime', 'forecast_totime']
        ).reset_index(drop=True)
        print(f"  æ’åºå®Œæˆ: {len(merged_df):,} è¡Œ")
        print(f"  forecast_fromtime èŒƒå›´: {merged_df['forecast_fromtime'].min()} ~ {merged_df['forecast_fromtime'].max()}")
        print(f"  forecast_totime èŒƒå›´: {merged_df['forecast_totime'].min()} ~ {merged_df['forecast_totime'].max()}")
    except Exception as e:
        print(f"  âš ï¸  æ’åºæ—¶å‡ºç°é—®é¢˜: {e}")
        print(f"  å°†ä¿å­˜æœªæ’åºçš„æ•°æ®")
    
    # ä¿å­˜æ–‡ä»¶
    print("\nä¿å­˜æ–‡ä»¶...")
    # ç¡®ä¿æ—¶é—´æ ¼å¼å®Œæ•´ä¿å­˜ï¼ˆåŒ…å«æ—¶åˆ†ç§’ï¼‰
    if 'forecast_fromtime' in merged_df.columns:
        merged_df['forecast_fromtime'] = merged_df['forecast_fromtime'].dt.strftime('%Y-%m-%d %H:%M:%S')
    if 'forecast_totime' in merged_df.columns:
        merged_df['forecast_totime'] = merged_df['forecast_totime'].dt.strftime('%Y-%m-%d %H:%M:%S')
    if 'actualvaluetime' in merged_df.columns and merged_df['actualvaluetime'].notna().any():
        merged_df['actualvaluetime'] = pd.to_datetime(merged_df['actualvaluetime'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')
    
    merged_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')
    
    file_size_mb = OUTPUT_FILE.stat().st_size / (1024 * 1024)
    print(f"  âœ… å·²ä¿å­˜: {OUTPUT_FILE.name}")
    print(f"  æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("åˆå¹¶å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print("="*80)
    print(f"æ€»è®°å½•æ•°: {len(merged_df):,} è¡Œ")
    print(f"æ€»åˆ—æ•°: {len(merged_df.columns)} åˆ—")
    print(f"åˆ—å: {merged_df.columns.tolist()}")
    
    if 'forecast_fromtime' in merged_df.columns:
        try:
            print(f"\nforecast_fromtime èŒƒå›´: {merged_df['forecast_fromtime'].min()} ~ {merged_df['forecast_fromtime'].max()}")
            print(f"forecast_totime èŒƒå›´: {merged_df['forecast_totime'].min()} ~ {merged_df['forecast_totime'].max()}")
        except:
            pass
    
    # æ˜¾ç¤ºå„åˆ—çš„ç©ºå€¼ç»Ÿè®¡
    print("\nç©ºå€¼ç»Ÿè®¡:")
    null_found = False
    for col in merged_df.columns:
        null_count = merged_df[col].isna().sum()
        if null_count > 0:
            null_pct = (null_count / len(merged_df)) * 100
            print(f"  {col}: {null_count:,} ({null_pct:.2f}%)")
            null_found = True
    if not null_found:
        print("  æ— ç©ºå€¼ âœ…")
    
    # æ˜¾ç¤º stationid åˆ†å¸ƒ
    if 'stationid' in merged_df.columns:
        print("\nstationid åˆ†å¸ƒ:")
        station_counts = merged_df['stationid'].value_counts().sort_index()
        for station_id, count in station_counts.items():
            pct = (count / len(merged_df)) * 100
            print(f"  stationid {station_id}: {count:,} ({pct:.2f}%)")
    
    # æ˜¾ç¤º forecast_fromtime åˆ†å¸ƒï¼ˆå‰10ä¸ªï¼‰
    if 'forecast_fromtime' in merged_df.columns:
        print("\nforecast_fromtime åˆ†å¸ƒï¼ˆå‰10ä¸ªï¼‰:")
        forecast_counts = pd.to_datetime(merged_df['forecast_fromtime']).value_counts().head(10)
        for forecast_time, count in forecast_counts.items():
            print(f"  {forecast_time}: {count:,} æ¡è®°å½•")
    
    print(f"\nè¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print("="*80)
    print("\nğŸ“‹ æ’åºè¯´æ˜ï¼š")
    print("  - å…ˆæŒ‰ forecast_fromtimeï¼ˆé¢„æµ‹èµ·æŠ¥æ—¶é—´ï¼‰æ’åº")
    print("  - å†æŒ‰ forecast_totimeï¼ˆé¢„æµ‹ç»“æŸæ—¶é—´ï¼‰æ’åº")
    print("  - è¿™æ ·å¯ä»¥çœ‹åˆ°æ¯æ¬¡é¢„æµ‹çš„å®Œæ•´æ—¶é—´åºåˆ—")
    print("="*80)


if __name__ == "__main__":
    try:
        merge_all()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

