"""
å…¨é¢éªŒè¯æ‰€æœ‰23ä¸ªæ•°æ®é›†
å¯¹æ¯”æŠ¥å‘Šæè¿°ä¸å®é™…æ•°æ®ï¼Œæ‰¾å‡ºæ‰€æœ‰å·®å¼‚
"""
import pandas as pd
import os
from pathlib import Path
from datetime import datetime
import json
import chardet

# æ•°æ®æºç›®å½•
SOURCE_DIR = Path(r"E:\å¾…å¤„ç†æ•°æ®")

# éªŒè¯ç»“æœ
validation_results = {
    "validation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "total_datasets": 23,
    "datasets": {}
}

def detect_encoding(file_path, sample_size=10240):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    try:
        with open(file_path, 'rb') as f:
            raw = f.read(sample_size)
        result = chardet.detect(raw)
        return result.get('encoding', 'utf-8') or 'utf-8'
    except:
        return 'utf-8'

def validate_dataset(dataset_name, expected_info):
    """éªŒè¯å•ä¸ªæ•°æ®é›†"""
    print(f"\n{'='*80}")
    print(f"éªŒè¯æ•°æ®é›†: {dataset_name}")
    print(f"{'='*80}")
    
    dataset_dir = SOURCE_DIR / dataset_name
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not dataset_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
        return {
            "status": "error",
            "error": "ç›®å½•ä¸å­˜åœ¨"
        }
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = sorted([f for f in dataset_dir.glob("*.csv")])
    
    if not csv_files:
        print(f"âš ï¸  ç›®å½•ä¸­æ²¡æœ‰CSVæ–‡ä»¶")
        return {
            "status": "warning",
            "warning": "æ²¡æœ‰CSVæ–‡ä»¶"
        }
    
    print(f"âœ… æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    result = {
        "status": "success",
        "file_count": len(csv_files),
        "sampled_files": [],
        "columns_found": {},
        "time_range": {},
        "issues": [],
        "quality_stats": {}
    }
    
    # æŠ½æ ·éªŒè¯ï¼ˆå‰3ä¸ªã€ä¸­é—´2ä¸ªã€æœ€å3ä¸ªï¼‰
    if len(csv_files) <= 8:
        sample_files = csv_files
    else:
        mid = len(csv_files) // 2
        sample_files = csv_files[:3] + csv_files[mid-1:mid+1] + csv_files[-3:]
    
    print(f"ğŸ“Š æŠ½æ ·éªŒè¯ {len(sample_files)} ä¸ªæ–‡ä»¶...")
    
    all_columns_sets = []
    time_values = []
    
    for i, file_path in enumerate(sample_files, 1):
        file_name = file_path.name
        print(f"\n  [{i}/{len(sample_files)}] {file_name}")
        
        try:
            # æ£€æµ‹ç¼–ç 
            encoding = detect_encoding(file_path)
            
            # è¯»å–æ–‡ä»¶ï¼ˆåªè¯»å‰100è¡Œç”¨äºå¿«é€ŸéªŒè¯ï¼‰
            df = pd.read_csv(file_path, encoding=encoding, nrows=100)
            
            # è®°å½•åˆ—å
            columns = df.columns.tolist()
            all_columns_sets.append(set(columns))
            
            print(f"      åˆ—æ•°: {len(columns)}")
            print(f"      åˆ—å: {columns[:5]}{'...' if len(columns) > 5 else ''}")
            
            # è®°å½•åˆ°ç»“æœ
            result["sampled_files"].append({
                "file": file_name,
                "columns": columns,
                "column_count": len(columns),
                "row_count_sample": len(df),
                "encoding": encoding
            })
            
            # å°è¯•æå–æ—¶é—´ä¿¡æ¯
            time_cols = [col for col in columns if 'time' in col.lower() or 'date' in col.lower()]
            if time_cols:
                for time_col in time_cols[:1]:  # åªå–ç¬¬ä¸€ä¸ªæ—¶é—´åˆ—
                    try:
                        time_series = pd.to_datetime(df[time_col])
                        if not time_series.isna().all():
                            time_values.extend(time_series.dropna().tolist())
                            print(f"      æ—¶é—´åˆ—: {time_col}")
                            print(f"      æ—¶é—´èŒƒå›´: {time_series.min()} ~ {time_series.max()}")
                    except:
                        pass
            
            # æ£€æŸ¥ç‰¹æ®Šå€¼
            for col in columns:
                if df[col].dtype in ['float64', 'int64']:
                    # æ£€æŸ¥-1å€¼
                    neg_one_count = (df[col] == -1).sum()
                    if neg_one_count > 0:
                        print(f"      âš ï¸  {col}: å‘ç° {neg_one_count} ä¸ª -1 å€¼")
                    
                    # æ£€æŸ¥0å€¼
                    zero_count = (df[col] == 0).sum()
                    if zero_count > len(df) * 0.5:  # è¶…è¿‡50%
                        print(f"      âš ï¸  {col}: {zero_count} ä¸ª 0 å€¼ ({zero_count/len(df)*100:.1f}%)")
                    
                    # æ£€æŸ¥NaN
                    nan_count = df[col].isna().sum()
                    if nan_count > 0:
                        print(f"      âš ï¸  {col}: {nan_count} ä¸ª NaN å€¼ ({nan_count/len(df)*100:.1f}%)")
            
        except Exception as e:
            print(f"      âŒ è¯»å–å¤±è´¥: {str(e)}")
            result["issues"].append({
                "file": file_name,
                "error": str(e)
            })
    
    # åˆ†æåˆ—çš„ä¸€è‡´æ€§
    if len(all_columns_sets) > 1:
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ–‡ä»¶åˆ—åä¸€è‡´
        first_cols = all_columns_sets[0]
        all_same = all(cols == first_cols for cols in all_columns_sets)
        
        if all_same:
            print(f"\nâœ… æ‰€æœ‰æŠ½æ ·æ–‡ä»¶åˆ—åä¸€è‡´")
            result["columns_found"]["consistent"] = True
            result["columns_found"]["columns"] = list(first_cols)
        else:
            print(f"\nâš ï¸  å‘ç°å¤šç‰ˆæœ¬è¡¨å¤´ï¼")
            result["columns_found"]["consistent"] = False
            
            # æ‰¾å‡ºæ‰€æœ‰ä¸åŒçš„åˆ—é›†åˆ
            unique_column_sets = []
            for cols in all_columns_sets:
                if cols not in unique_column_sets:
                    unique_column_sets.append(cols)
            
            print(f"   å…±å‘ç° {len(unique_column_sets)} ç§ä¸åŒçš„åˆ—ç»„åˆ:")
            for idx, cols in enumerate(unique_column_sets, 1):
                print(f"   ç‰ˆæœ¬{idx}: {sorted(cols)}")
            
            # æ‰¾å‡ºå·®å¼‚åˆ—
            all_cols = set()
            for cols in all_columns_sets:
                all_cols.update(cols)
            
            common_cols = set.intersection(*all_columns_sets)
            diff_cols = all_cols - common_cols
            
            print(f"\n   å…±åŒåˆ— ({len(common_cols)}): {sorted(common_cols)}")
            print(f"   å·®å¼‚åˆ— ({len(diff_cols)}): {sorted(diff_cols)}")
            
            result["columns_found"]["versions"] = [list(cols) for cols in unique_column_sets]
            result["columns_found"]["common_columns"] = list(common_cols)
            result["columns_found"]["diff_columns"] = list(diff_cols)
            
            result["issues"].append({
                "type": "multiple_versions",
                "description": f"å‘ç°{len(unique_column_sets)}ç§ä¸åŒçš„åˆ—ç»„åˆ",
                "diff_columns": list(diff_cols)
            })
    
    # æ—¶é—´èŒƒå›´ç»Ÿè®¡
    if time_values:
        result["time_range"]["min"] = str(min(time_values))
        result["time_range"]["max"] = str(max(time_values))
        print(f"\nğŸ“… æ•´ä½“æ—¶é—´èŒƒå›´: {result['time_range']['min']} ~ {result['time_range']['max']}")
    
    return result


# å®šä¹‰æ‰€æœ‰23ä¸ªæ•°æ®é›†åŠå…¶é¢„æœŸä¿¡æ¯
DATASETS = {
    "all_baseinfo": {
        "expected_files": 211,
        "expected_columns": ["datetime", "pvthopower", "wpthopower"],
        "time_range": "20241211-20250709",
        "granularity": "åˆ†é’Ÿçº§"
    },
    "all_day_powergeneration": {
        "expected_files": 9,
        "expected_columns": ["datetime", "allproducequantity", "allselfusequantity"],
        "time_range": "202411-202507",
        "granularity": "æ—¥åº¦"
    },
    "all_thopower": {
        "expected_files": 12,
        "expected_columns": ["datetime", "pvthopower", "wpthopower"],
        "time_range": "20241130-20241211",
        "granularity": "åˆ†é’Ÿçº§"
    },
    "bay_day_powergeneration": {
        "expected_files": 9,
        "expected_columns": ["datetime", "bayid", "quantity"],
        "time_range": "202411-202507",
        "granularity": "æ—¥åº¦"
    },
    "computeschedule": {
        "expected_files": 2,
        "expected_columns": ["totalanalogid", "computedatetime"],
        "time_range": "2024-12-12 ~ 2025-07-09",
        "granularity": "æ‰¹æ¬¡å¿«ç…§"
    },
    "his_node": {
        "expected_files": 5,
        "expected_columns": ["hostname", "lastUpdateTime"],
        "time_range": "2024-10-17 ~ 2025-07-09",
        "granularity": "ç›‘æ§é‡‡æ ·"
    },
    "hisaccumulator": {
        "expected_files": 24,
        "expected_columns": ["SAVETIME", "ACCUMULATORID"],
        "time_range": "20240101-20241109",
        "granularity": "è®°å½•"
    },
    "hisagcavcstation": {
        "expected_files": 1,
        "expected_columns": ["stationid", "savetime"],
        "time_range": "2024-11-25 ~ 2024-12-26",
        "granularity": "å‚æ•°å¿«ç…§"
    },
    "hisstsgbatst": {
        "expected_files": 2,
        "expected_columns": ["id", "savetime", "todayinpt"],
        "time_range": "2024-11-21 ~ 2025-07-09",
        "granularity": "æ—¥åº¦ç»Ÿè®¡"
    },
    "hisstsgbatstation": {
        "expected_files": 2,
        "expected_columns": ["id", "savetime", "todayinpt"],
        "time_range": "2024-11-21 ~ 2025-07-09",
        "granularity": "æ—¥åº¦ç»Ÿè®¡"
    },
    "inverter_day_powergeneration": {
        "expected_files": 9,
        "expected_columns": ["datetime", "devid", "accumulatorid", "quantity"],
        "time_range": "202411-202507",
        "granularity": "æ—¥åº¦"
    },
    "powerforecast_fd_weatherdata": {
        "expected_files": 211,
        "expected_columns": ["id", "savetime", "datatime", "temperature"],
        "time_range": "20241025-20250709",
        "granularity": "5åˆ†é’Ÿ"
    },
    "powerforecast_fd_weatherforecast": {
        "expected_files": 187,
        "expected_columns": ["id", "savetime", "datatime", "temperature"],
        "time_range": "20241202-20250709",
        "granularity": "15åˆ†é’Ÿ"
    },
    "powerforecast_powerstat": {
        "expected_files": 10,
        "expected_columns": ["id", "savetime", "statistictime"],
        "time_range": "202410-202507",
        "granularity": "ç»Ÿè®¡"
    },
    "powerforecast_short": {
        "expected_files": 199,
        "expected_columns": ["id", "savetime", "forecastvaluetime"],
        "time_range": "20241026-20250709",
        "granularity": "15åˆ†é’Ÿ"
    },
    "powerforecast_ultrashort": {
        "expected_files": 210,
        "expected_columns": ["id", "savetime", "forecast_fromtime"],
        "time_range": "20241026-20250709",
        "granularity": "16æ­¥é¢„æµ‹"
    },
    "station_baseinfo": {
        "expected_files": 211,
        "expected_columns": ["datetime", "stationid", "thopower", "aevwindspeed"],
        "time_range": "20241211-20250709",
        "granularity": "åˆ†é’Ÿçº§"
    },
    "station_day_powergeneration": {
        "expected_files": 9,
        "expected_columns": ["datetime", "stationid", "producequantity"],
        "time_range": "202411-202507",
        "granularity": "æ—¥åº¦"
    },
    "station_thopower": {
        "expected_files": 12,
        "expected_columns": ["datetime", "stationid", "thopower"],
        "time_range": "20241130-20241211",
        "granularity": "åˆ†é’Ÿçº§"
    },
    "windturbine_1min_windspeed": {
        "expected_files": 223,
        "expected_columns": ["datetime", "devid", "analog_id", "aevwindspeed"],
        "time_range": "20241129-20250709",
        "granularity": "1åˆ†é’Ÿ"
    },
    "windturbine_day_powergeneration": {
        "expected_files": 9,
        "expected_columns": ["datetime", "devid", "accumulatorid", "quantity"],
        "time_range": "202411-202507",
        "granularity": "æ—¥åº¦"
    },
    "windturbine_statusstat": {
        "expected_files": 3,
        "expected_columns": ["windturbineid", "stattime", "savetime"],
        "time_range": "2024-12 ~ 2025-07",
        "granularity": "æœˆåº¦/å¹´åº¦"
    },
    "wtoscstmsg": {
        "expected_files": 7,
        "expected_columns": ["id", "WTNAME", "STARTTIME"],
        "time_range": "202407-202507",
        "granularity": "äº‹ä»¶è®°å½•"
    }
}


def main():
    """ä¸»éªŒè¯æµç¨‹"""
    print("="*80)
    print("å¼€å§‹å…¨é¢éªŒè¯æ‰€æœ‰23ä¸ªæ•°æ®é›†")
    print("="*80)
    print(f"æ•°æ®æºç›®å½•: {SOURCE_DIR}")
    print(f"éªŒè¯æ—¶é—´: {validation_results['validation_time']}")
    print()
    
    # é€ä¸ªéªŒè¯
    for dataset_name, expected_info in DATASETS.items():
        result = validate_dataset(dataset_name, expected_info)
        validation_results["datasets"][dataset_name] = result
        
        # å¯¹æ¯”é¢„æœŸ
        print(f"\nğŸ“‹ å¯¹æ¯”é¢„æœŸä¿¡æ¯:")
        print(f"   é¢„æœŸæ–‡ä»¶æ•°: {expected_info['expected_files']}")
        print(f"   å®é™…æ–‡ä»¶æ•°: {result.get('file_count', 0)}")
        
        if result.get('file_count', 0) != expected_info['expected_files']:
            print(f"   âš ï¸  æ–‡ä»¶æ•°ä¸åŒ¹é…ï¼")
            result["issues"].append({
                "type": "file_count_mismatch",
                "expected": expected_info['expected_files'],
                "actual": result.get('file_count', 0)
            })
    
    # ä¿å­˜éªŒè¯ç»“æœ
    output_file = Path(__file__).parent.parent / "data_validation_results" / f"validation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(validation_results, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*80)
    print("éªŒè¯å®Œæˆï¼")
    print("="*80)
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç»Ÿè®¡æ±‡æ€»
    total = len(validation_results["datasets"])
    success = sum(1 for r in validation_results["datasets"].values() if r["status"] == "success")
    with_issues = sum(1 for r in validation_results["datasets"].values() if r.get("issues"))
    
    print(f"\nğŸ“Š éªŒè¯æ±‡æ€»:")
    print(f"   æ€»æ•°æ®é›†: {total}")
    print(f"   éªŒè¯æˆåŠŸ: {success}")
    print(f"   å‘ç°é—®é¢˜: {with_issues}")
    
    # åˆ—å‡ºæ‰€æœ‰æœ‰é—®é¢˜çš„æ•°æ®é›†
    if with_issues > 0:
        print(f"\nâš ï¸  æœ‰é—®é¢˜çš„æ•°æ®é›†:")
        for dataset_name, result in validation_results["datasets"].items():
            if result.get("issues"):
                print(f"\n   {dataset_name}:")
                for issue in result["issues"]:
                    print(f"      - {issue.get('type', 'unknown')}: {issue.get('description', issue.get('error', 'N/A'))}")


if __name__ == "__main__":
    main()

